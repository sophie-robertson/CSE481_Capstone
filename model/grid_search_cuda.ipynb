{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from math import ceil\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "g_cuda = torch.Generator(device='cuda:1')\n",
    "from mrnn7 import MilliesDataset, MilliesRNN\n",
    "from hessianfree import HessianFree\n",
    "\n",
    "# Load data\n",
    "whole_dataset = MilliesDataset('monkey_data.mat')\n",
    "dataset_size = len(whole_dataset)\n",
    "train_dataset, test_dataset = random_split(whole_dataset, [401, 101])\n",
    "\n",
    "in_dim, out_dim, trial_len = whole_dataset.dimensions() #  21  &  50\n",
    "hid_dim = 100\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=ceil(len(train_dataset)/5), shuffle=False, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, pin_memory=True)\n",
    "# for generating output later\n",
    "whole_dataloader = DataLoader(whole_dataset, batch_size = len(whole_dataset), shuffle=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When i = 0, clipping, when i = 1, no clipping\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching over models...:   0%|                                                                                                                | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00011547772896171797\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.00012468100331797458\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00011389508917190061\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00012245388830652332\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00012449977347756377\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00011445352309706188\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00012295301265940808\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.3333333333333333, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00013399424750616055\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00015690824994356325\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.00011097081927674832\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.0001203827098897188\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00011703552733553518\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00011513527888472718\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00011320513590137557\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00011400774231936672\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.25, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001130196090676997\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00011268816888332367\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.00011482547120292588\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00014910891209498492\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00011404695929867206\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00014103119178573683\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.0001192190822693381\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001275337365741777\n",
      "\n",
      "Learning rate = 0.01, batch size = 0.2, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001471088645924436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching over models...:  33%|█████████████████████████████████▎                                                                  | 1/3 [34:31<1:09:03, 2071.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00011370313529035832\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 7.960990550789503e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 6.709510626474229e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 6.687808966282571e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.0001239498597708079\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 9.318949230531655e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00011842954203043834\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.3333333333333333, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00016237070581110396\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 6.263819292630299e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 9.043794125318527e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00010298228463028917\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 8.42728808817297e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00014758499173244628\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00012322793723923146\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 6.748079829434358e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.25, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 6.844522431492805e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 7.662532094976689e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 6.518416604635739e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 8.088161265200908e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00011638552872556271\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 6.016962562162097e-05\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.0001569343085336213\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00015462117988874417\n",
      "\n",
      "Learning rate = 0.001, batch size = 0.2, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00012415254691449724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching over models...:  67%|██████████████████████████████████████████████████████████████████▋                                 | 2/3 [1:08:46<34:21, 2061.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00017771377495609888\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.00017872264627182837\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00017957743441704475\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.000184317810995744\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00017901633561837792\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00018464036212109103\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001865277749181974\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.3333333333333333, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00018219963306247597\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00017661514627461387\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.0001697889390853372\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00017433418053211552\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.0001742124631263242\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00017670761461895291\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00017384312456787223\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001694752401349568\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.25, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00017871300108952096\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 0, weight decay = 0\n",
      "\n",
      "Average test: 0.00016330896258944332\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 1, weight decay = 0\n",
      "\n",
      "Average test: 0.00015869668435932385\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 0, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.00016851374639733003\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 1, weight decay = 1e-06\n",
      "\n",
      "Average test: 0.0001579024045184107\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 0, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.0001743009848759906\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 1, weight decay = 1e-05\n",
      "\n",
      "Average test: 0.00016513785882161396\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 0, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.00016764601194622493\n",
      "\n",
      "Learning rate = 0.0001, batch size = 0.2, i = 1, weight decay = 0.0001\n",
      "\n",
      "Average test: 0.0001774615617376743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching over models...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [1:42:54<00:00, 2058.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "from tqdm import tqdm\n",
    "\n",
    "from mrnn7 import MilliesRNN\n",
    "from hessianfree import HessianFree\n",
    "\n",
    "hessian = False\n",
    "hardcore = True\n",
    "intermodule_connections_removed = .9\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_size = [3,4,5]\n",
    "weight_decay = [0,1e-6,1e-5,1e-4]\n",
    "\n",
    "num_epochs = 20\n",
    "test_losses = {}\n",
    "\n",
    "print(\"When i = 0, clipping, when i = 1, no clipping\\n\")\n",
    "\n",
    "criterion1 = nn.MSELoss() \n",
    "\n",
    "for l in tqdm(learning_rates, desc=\"Searching over models...\"):\n",
    "    for b in batch_size:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=ceil(len(train_dataset)/b), shuffle=True)\n",
    "        for w in weight_decay:\n",
    "            for i in range(2):\n",
    "                print(\"Learning rate = \" + str(l) + \", batch size = \" + str(1/b) + \", i = \" + str(i) + \", weight decay = \" + str(w) + \"\\n\")\n",
    "                model_type = f\"mse_{l}_{b}_{i}_{w}\"\n",
    "                model = MilliesRNN(in_dim, hid_dim, out_dim, True)\n",
    "                model.to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=l, weight_decay = w)\n",
    "                module1 = model.h2o\n",
    "                prune.random_unstructured(module1, name=\"weight\", amount=intermodule_connections_removed)\n",
    "                module2 = model.thal\n",
    "                prune.random_unstructured(module2, name=\"weight\", amount=intermodule_connections_removed)\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    for i, (inp_batch, out_batch) in enumerate(train_dataloader):\n",
    "                        inp_batch = inp_batch.to(device)\n",
    "                        out_batch = out_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        outputs = model(inp_batch)   \n",
    "\n",
    "                        loss = criterion1(outputs, out_batch)\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if(i == 0):\n",
    "                            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                \n",
    "\n",
    "                num_samples = len(test_dataset)\n",
    "            \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    inp, out = next(iter(test_dataloader))\n",
    "                    gen_out = model(inp.to(device))\n",
    "                    loss1 = criterion1(gen_out, out.to(device))\n",
    "\n",
    "                \n",
    "                test_losses[model_type] = (loss1.item() / num_samples)\n",
    "                print(f\"Average test: {test_losses[model_type]}\\n\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inp, out_true = next(iter(whole_dataloader))\n",
    "                    whole_out = model(inp.to(device))\n",
    "\n",
    "                with open(f'tuning_outputs2/{model_type}.pickle', 'wb') as handle:\n",
    "                    pickle.dump(whole_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse_0.01_3_0_0': 0.00011547772896171797, 'mse_0.01_3_1_0': 0.00012468100331797458, 'mse_0.01_3_0_1e-06': 0.00011389508917190061, 'mse_0.01_3_1_1e-06': 0.00012245388830652332, 'mse_0.01_3_0_1e-05': 0.00012449977347756377, 'mse_0.01_3_1_1e-05': 0.00011445352309706188, 'mse_0.01_3_0_0.0001': 0.00012295301265940808, 'mse_0.01_3_1_0.0001': 0.00013399424750616055, 'mse_0.01_4_0_0': 0.00015690824994356325, 'mse_0.01_4_1_0': 0.00011097081927674832, 'mse_0.01_4_0_1e-06': 0.0001203827098897188, 'mse_0.01_4_1_1e-06': 0.00011703552733553518, 'mse_0.01_4_0_1e-05': 0.00011513527888472718, 'mse_0.01_4_1_1e-05': 0.00011320513590137557, 'mse_0.01_4_0_0.0001': 0.00011400774231936672, 'mse_0.01_4_1_0.0001': 0.0001130196090676997, 'mse_0.01_5_0_0': 0.00011268816888332367, 'mse_0.01_5_1_0': 0.00011482547120292588, 'mse_0.01_5_0_1e-06': 0.00014910891209498492, 'mse_0.01_5_1_1e-06': 0.00011404695929867206, 'mse_0.01_5_0_1e-05': 0.00014103119178573683, 'mse_0.01_5_1_1e-05': 0.0001192190822693381, 'mse_0.01_5_0_0.0001': 0.0001275337365741777, 'mse_0.01_5_1_0.0001': 0.0001471088645924436, 'mse_0.001_3_0_0': 0.00011370313529035832, 'mse_0.001_3_1_0': 7.960990550789503e-05, 'mse_0.001_3_0_1e-06': 6.709510626474229e-05, 'mse_0.001_3_1_1e-06': 6.687808966282571e-05, 'mse_0.001_3_0_1e-05': 0.0001239498597708079, 'mse_0.001_3_1_1e-05': 9.318949230531655e-05, 'mse_0.001_3_0_0.0001': 0.00011842954203043834, 'mse_0.001_3_1_0.0001': 0.00016237070581110396, 'mse_0.001_4_0_0': 6.263819292630299e-05, 'mse_0.001_4_1_0': 9.043794125318527e-05, 'mse_0.001_4_0_1e-06': 0.00010298228463028917, 'mse_0.001_4_1_1e-06': 8.42728808817297e-05, 'mse_0.001_4_0_1e-05': 0.00014758499173244628, 'mse_0.001_4_1_1e-05': 0.00012322793723923146, 'mse_0.001_4_0_0.0001': 6.748079829434358e-05, 'mse_0.001_4_1_0.0001': 6.844522431492805e-05, 'mse_0.001_5_0_0': 7.662532094976689e-05, 'mse_0.001_5_1_0': 6.518416604635739e-05, 'mse_0.001_5_0_1e-06': 8.088161265200908e-05, 'mse_0.001_5_1_1e-06': 0.00011638552872556271, 'mse_0.001_5_0_1e-05': 6.016962562162097e-05, 'mse_0.001_5_1_1e-05': 0.0001569343085336213, 'mse_0.001_5_0_0.0001': 0.00015462117988874417, 'mse_0.001_5_1_0.0001': 0.00012415254691449724, 'mse_0.0001_3_0_0': 0.00017771377495609888, 'mse_0.0001_3_1_0': 0.00017872264627182837, 'mse_0.0001_3_0_1e-06': 0.00017957743441704475, 'mse_0.0001_3_1_1e-06': 0.000184317810995744, 'mse_0.0001_3_0_1e-05': 0.00017901633561837792, 'mse_0.0001_3_1_1e-05': 0.00018464036212109103, 'mse_0.0001_3_0_0.0001': 0.0001865277749181974, 'mse_0.0001_3_1_0.0001': 0.00018219963306247597, 'mse_0.0001_4_0_0': 0.00017661514627461387, 'mse_0.0001_4_1_0': 0.0001697889390853372, 'mse_0.0001_4_0_1e-06': 0.00017433418053211552, 'mse_0.0001_4_1_1e-06': 0.0001742124631263242, 'mse_0.0001_4_0_1e-05': 0.00017670761461895291, 'mse_0.0001_4_1_1e-05': 0.00017384312456787223, 'mse_0.0001_4_0_0.0001': 0.0001694752401349568, 'mse_0.0001_4_1_0.0001': 0.00017871300108952096, 'mse_0.0001_5_0_0': 0.00016330896258944332, 'mse_0.0001_5_1_0': 0.00015869668435932385, 'mse_0.0001_5_0_1e-06': 0.00016851374639733003, 'mse_0.0001_5_1_1e-06': 0.0001579024045184107, 'mse_0.0001_5_0_1e-05': 0.0001743009848759906, 'mse_0.0001_5_1_1e-05': 0.00016513785882161396, 'mse_0.0001_5_0_0.0001': 0.00016764601194622493, 'mse_0.0001_5_1_0.0001': 0.0001774615617376743}\n"
     ]
    }
   ],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'tuning_outputs2/test_loss_mse.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_losses, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse_0.001_5_0_1e-05', 6.016962562162097e-05)\n",
      "('mse_0.001_4_0_0', 6.263819292630299e-05)\n",
      "('mse_0.001_5_1_0', 6.518416604635739e-05)\n",
      "('mse_0.001_3_1_1e-06', 6.687808966282571e-05)\n",
      "('mse_0.001_3_0_1e-06', 6.709510626474229e-05)\n",
      "('mse_0.001_4_0_0.0001', 6.748079829434358e-05)\n",
      "('mse_0.001_4_1_0.0001', 6.844522431492805e-05)\n",
      "('mse_0.001_5_0_0', 7.662532094976689e-05)\n",
      "('mse_0.001_3_1_0', 7.960990550789503e-05)\n",
      "('mse_0.001_5_0_1e-06', 8.088161265200908e-05)\n",
      "('mse_0.001_4_1_1e-06', 8.42728808817297e-05)\n",
      "('mse_0.001_4_1_0', 9.043794125318527e-05)\n"
     ]
    }
   ],
   "source": [
    "asc_sorted_losses = sorted(test_losses.items(), key=lambda x:x[1])\n",
    "for i,k in enumerate(asc_sorted_losses):\n",
    "    print(k)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse_0.0001_3_0_0.0001', 0.0001865277749181974)\n",
      "('mse_0.0001_3_1_1e-05', 0.00018464036212109103)\n",
      "('mse_0.0001_3_1_1e-06', 0.000184317810995744)\n",
      "('mse_0.0001_3_1_0.0001', 0.00018219963306247597)\n",
      "('mse_0.0001_3_0_1e-06', 0.00017957743441704475)\n",
      "('mse_0.0001_3_0_1e-05', 0.00017901633561837792)\n",
      "('mse_0.0001_3_1_0', 0.00017872264627182837)\n",
      "('mse_0.0001_4_1_0.0001', 0.00017871300108952096)\n",
      "('mse_0.0001_3_0_0', 0.00017771377495609888)\n",
      "('mse_0.0001_5_1_0.0001', 0.0001774615617376743)\n",
      "('mse_0.0001_4_0_1e-05', 0.00017670761461895291)\n",
      "('mse_0.0001_4_0_0', 0.00017661514627461387)\n"
     ]
    }
   ],
   "source": [
    "desc_sorted_losses = sorted(test_losses.items(), key=lambda x:x[1], reverse=True)\n",
    "for i,k in enumerate(desc_sorted_losses):\n",
    "    print(k)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
